\documentclass[twocolumn, a4paper]{article}
\usepackage[margin=0.7in, top=1.0in, bottom=0.7in, columnsep=0.5in]{geometry} % 进一步缩小页边距
\usepackage{graphicx} % 插入图片
\usepackage{amsmath, amssymb} % 数学公式
\usepackage{amsbsy} % 数学字体
\usepackage{titling} % 标题设置
\usepackage{float} % 图片浮动控制
\usepackage{lipsum} % 生成示例文本
\usepackage{enumitem} % 列表设置
\usepackage{hyperref} % 超链接
\usepackage{caption} % 图表标题
\usepackage{subcaption} % 子图表
\usepackage{setspace} % 行间距控制
\usepackage{fancyhdr} % 页眉页脚
\usepackage{url}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{listings}
\lstset{
    basicstyle=\ttfamily\footnotesize,
    frame=single,
    numbers=left,
    numberstyle=\tiny,
    tabsize=2,
    breaklines=true,
    showstringspaces=false
}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue, 
    urlcolor=blue,
    pdfborder={0 0 0}  % 关键：去掉边框
}



% 页眉设置 - 更突出
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\bfseries Group: Monte Carlo Gamblers} % 左侧页眉，加粗突出
\fancyhead[R]{\small\bfseries Games on Integers} % 右侧页眉，加粗突出
\fancyhead[C]{\small\bfseries 2025 MCM} % 中间页眉，加粗突出
\renewcommand{\headrulewidth}{0.6pt} % 加粗页眉下划线
\fancyfoot[C]{\thepage} % 页脚居中显示页码
\fancyheadoffset{0pt}

% 确保首页页眉一致
\fancypagestyle{plain}{
    \fancyhf{}
    \fancyhead[L]{\small\bfseries Group: Monte Carlo Gamblers}
    \fancyhead[R]{\small\bfseries Games on Integers}
    \fancyhead[C]{\small\bfseries 2025 MCM}
    \renewcommand{\headrulewidth}{0.6pt} % 保持页眉下划线
    \fancyfoot[C]{\thepage}
}

% 紧凑的图表标题格式
\captionsetup{font=footnotesize, labelfont=bf, skip=0.3em}

% 更紧凑的段落和行间距
\setlength{\parskip}{0.2em}
\linespread{0.6} % 单倍行距
\setlength{\parindent}{1em} % 段落缩进

% 减小列表间距
\setlist{topsep=0.2em, partopsep=0em, itemsep=0.1em, parsep=0em}

% 标题设置 - 更紧凑
\pretitle{\begin{center}\Large\bfseries}
\posttitle{\end{center}\vskip -0.3em}
\title{Ant Colony Optimization (ACO) and Multi-Agent Adversarial Reinforcement Learning (MAARF) in the 1--100 Divisibility Chain Game}

% 作者信息（四个成员区块，更紧凑）
\author{
    \begin{minipage}[t]{0.23\textwidth}
        \centering
        \small\bfseries Jincan LI\\
        \tiny 50032637\\
        \tiny jli843@connect.hkust-gz.edu.cn
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.23\textwidth}
        \centering
        \small\bfseries Wenlue CHAI\\
        \tiny 50035926\\
        \tiny wchai181@connect.hkust-gz.edu.cn
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.23\textwidth}
        \centering
        \small\bfseries Zheng TIAN\\
        \tiny 50035354\\
        \tiny ztian117@connect.hkust-gz.edu.cn
    \end{minipage}
    \hfill
    \begin{minipage}[t]{0.23\textwidth}
        \centering
        \small\bfseries Zixian GUO\\
        \tiny 50033746\\
        \tiny zguo849@connect.hkust-gz.edu.cn
    \end{minipage}
}

\date{} % 不显示日期
\predate{}
\postdate{}

\begin{document}

\maketitle


% 正文部分
\section{Introduction}
The 2025 MCM Junior ``Game on Integers'' problems involves two scenarios: 
\begin{itemize}
    \item \textbf{Cooperative Scenario}: Maximizing the total reward via a valid integer chain, 1-100, with consecutive divisibility.
    \item \textbf{Competitive scenario}: A two-player zero-sum game where Alice aims to maximize her score against Bob.
\end{itemize}
To address these challenges, we modeled the proble as an \textbf{Undirected Divisible Graph}:and employed:
\begin{itemize}
    \item \textbf{Graph Theory and Ant Colony Optimization (ACO)} for the cooperative scemario 
    \item \textbf{Mathematical Analysis and Reinforcement Learning (RL)} for the competitive scenario.
\end{itemize}

For the cooperate scenario, the graph converts the ``chain building'' into a ``maximum-sum path-finding'' task. Our implemention ACO achieved a high reward of \textbf{3551}; we verified its performance via the Hierarchical Assessment Method demonstrating its excellence.

For the competitive scenario, we model the game as a sequential two-player zero-sum game. We first employed the mathematical ways, finding a brilliant opening strategy, given the large latter state space where classical methods are imprctical, we propose a RM framework where agents Alice and Bob are trained through self-play to discover effective strategies. Our approach focuses on learning strategic patterns, enabling scalable and interpretable policy discovery.

The rest of the report is structured as follows: Section 2 details problem analysis and graph modeling. Section 3 covers the ACO solution for the cooperative scenario (modeling + evaluation). Section 4 discusses the mathematical thinking and reinforcement learning method for the competitive scenario (modeling + evaluation). Section 5 concludes and lists references.
\section{Problem Decomposition and Problem-Graph Modeling}
This section formalizes the game's core rules, scenario goals, and graph mapping. 
\subsection{Key Assumptions}
We assume that all players are perfectly rational, strictly adhere to game rules, have complete information about the graph topology, and aim to maximize their total rewards in Q (1), and maximize their respective rewards in Q (2).
\subsection{Core Game Rules}
A valid chain $X=(X_1, X_2, \dots, X_m)$, $k=1, 2, \dots,m$ must satisfy 3 non-negotiable constrains:
\begin{itemize}
    \item Every $X_k\in [1, 100]$, $X_k\in \mathbb{N} ^+$
    \item All $X_k$ are distinct.
    \item For consecutive $X_k$ and $X_{k+1}$, $X_k | X_{k+1}$ or $X_{k+1} | X_k$.
\end{itemize}
The game ends when no valid next pick.
\subsection{Scenario-Specific Goals}
For the cooperative scenario: Maximize total reward:$S_{total}=\sum_{k=1}^{m}X_i$
Key focus: Balancing ``keep the chain long'' and ``include high-value integers''.

For the competitive scenario: Alice and Bob alternate turns, each aiming to maximize their own total reward:
\begin{itemize}
    \item Alice's goal: Maximize $S_{2A}=\sum_{odd i}X_i$
    \item Bob's goal: Maximize $S_{2B}=\sum_{even j}X_j$
    \item overall goal: make sure: $S_{2A}>S_{2B}$
\end{itemize}
\subsection{Unified Graph Mapping}
All game elements map to an undirected divisibility graph $G=(V, E)$: $V={v_1, v_2, \dots,v_{100}}$, where $V$ is the node set, with $v_i$ representing $i\in \{1, \dots, 100\}$; $E$ contains $e_{i,j}$, which exists if $i | j$ or $j | i$.
A valid chain $X=\{X_1, X_2, \dots, X_m\}$ corresponds to a simple path $P=(v_{X_1}, v_{X_2}, \dots, v_{X_m})$ in $G$, The ``simple path'' property (no repeated nodes) derectly enforces the game's ``no duplicate integers'' constraint. The edge condition ensures the divisibility requirement is met. 
For the cooperative scenario, each node $v_i$ has weight $w(v_i)=i$, and we need to calculate total reward $S_1=\sum_{k=1}^{m}w(v_{X_k})$. In competition, Alice's reward $S_A$ weights of nodes in odd position of $P$; Bob's reward $S_B$ sums even positions.

The graph model collapse the game's complexityinto a single, computable structure by translationgevery rules into topological constraints, making the vague ``chain building'' task into a well-defined `` path-finding'' problem (a class of problems with mature algorithm solutions).


\section{Solution to the Cooperative Game Scenario (1)}
\subsection{Trial on DFS + Optimized Pruning}
Inspired by De Geest et al. (2020)'s OEIS A337125 (longest 77-element chain via pruned DFS) \textsuperscript{\cite{degeest2020a337125}}, we took this approach as a natrual starting point. However, the limitations---algorithmic bias toward length, subpar sums, and the unreliability of manual fixes---led us to abandon DFS+pruning.

% However, it flaws for our sum-maximization goal.
% \begin{itemize}
%     \item \textbf{Suboptimal sums}: The pruned DFS results underperformed even simple manual chains.
%     \item \textbf{Inherent bias}: Pruning rules favored longer paths with small integers, conflicting with our need to prioritize high values.
%     \item \textbf{Unreliability of manual enumeration}: While manual chains occasionally outperformed the algorithm, they were ad-hoc, non-reproducible, and limited by human intuition.
% \end{itemize}
% in favor of a method better suited for weighted optimization: Ant Colony Optimization (ACO).

\subsection{Model Establishing: Ant Colony Optimization (ACO) --- Optimal Performer}
ACO was selected for its ability to balance exploration (uncovering new paths) and exploitation (refining high-sum paths)---a critical fit for our sum-maximization objective.(implementation details in Appendix~\ref{app:aco_code})
\subsubsection{Core Mechanism}
ACO simulates ant behavior, with artificial ants traversing graph $G$ to build valid chains, guided by pheromones (collective memory) and heuristics (priorities):
\begin{itemize}
    \item \textbf{Path Construction}: Each ant starts at a random node, iteratively moving to adjacent, unused nodes (following edges $e_{i,j} \in E$) until no valid extensions exist. This strictly enforces the game's ``no-repeat'' and ``divisibility'' constraints without explicit checks.
    \item \textbf{Pheromone Trails $(\tau_{i,j})$}: Encode edge quality—high-sum chains deposit more pheromone (\(\Delta \tau_{i,j}=Q/S_{\text{ant}}\), $Q$=deposition constant), while evaporation (\(\tau_{i,j}=(1-\rho)\tau_{i,j}\), \(\rho\)=rate) avoids stagnation.
    \item \textbf{Heuristic and Transition Probability}:  $\eta_{i,j}=j$ biases ants toward large integers. The transition probability $$p_{i,j} = \frac{ \left\{ \tau_{i,j} \right\}^{\alpha} \cdot \left\{ \eta_{i,j} \right\}^{\beta} }{ \sum_{k \in \text{unused, adjacent}} \left\{ \tau_{i,k} \right\}^{\alpha} \cdot \left\{ \eta_{i,k} \right\}^{\beta} }$$ uses $\alpha$ (pheromone weight) and $\beta$ (heuristic weight) to balance memory and priorities.
\end{itemize}
% Alternative expressions:
% Ant Colony Optimization (ACO) is inherently suited for the "max-sum divisibility chain" problem—it outperforms brute-force search (avoids combinatorial explosion), greedy algorithms (escapes local optima), and genetic algorithms (naturally satisfies constraints). Its superiority comes from foraging-inspired mechanisms and tight alignment with the divisibility graph, as condensed below.
% As we denote the Pheromone Trails as $\tau_{i,j}$, Pheromone Evaporation Rate as $\rho$ ($\rho\in (0,1)$), and Pheromone Deposition Constant as $Q$, edges accumulate pheromones proportional to the quality (sum) of chains using them. After each iteration, pheromones evaporate slightly (\(\tau_{i,j} = (1-\rho)\tau_{i,j}\)) to avoid stagnation, while high-sum chains deposit additional pheromones (\(\tau_{i,j} += Q/S_{ant}\), where \(S_{ant}\) is the chain sum).
% To prioritize high-value integers, the heuristic factor is defined as $\eta_{i,j} = w(v_j) = j$  (destination node's value), biasing ants toward larger numbers.
% The transition probability for an ant moving from $v_i$ to $v_j$ is:
% \[
% p_{i,j} = \frac{\left\{\tau_{i,j}\right\}^{\{\alpha\}} \cdot \left\{\eta_{i,j}\right\}^{\{\beta\}}}{\left\{\sum_{k \in \text{unused, adjacent}} \left\{\tau_{i,k}\right\}^{\{\alpha\}} \cdot \left\{\eta_{i,k}\right\}^{\{\beta\}}\right\}}
% \]


\subsubsection{Optimized Parameters}
Key parameters were fine-tuned through extensive experimentation:
\begin{table}[htbp]
  \centering
  \setlength{\tabcolsep}{4pt}
  \caption{parameters}
\begin{tabular}{@{}lc@{}}
    \toprule
    \textbf{parameters}   & \textbf{10 Runs} \\
    \midrule
Number of Ants            &200\\
Iterations:                 &1000\\
Pheromone Evaporation Rate ($\rho$): &0.3\\
Heuristic Importance ($\beta$):    &2\\
Pheromone Importance ($\alpha$): &1 \\
    \bottomrule
  \end{tabular}
  \label{tab:reliability}
\end{table}

\subsubsection{Key Results}
After running the ACO algorithm with the optimized parameters,we obtained results as below:

The highest reward was \textbf{3551}. For the detailed code and sequence, please refer to Appendix 1.

\subsection{Model Evaluation and Reflection}
\subsubsection{Hierarchical Validation}
To evaluate the ACO model's effectiveness, we applied a three-layer hierarchical assessment method:
\begin{table}[htbp]
  \centering
  \setlength{\tabcolsep}{4pt} % Narrow column spacing
  \caption{Layer 1: Core Outcome Validation}
  \begin{tabular}{@{}lc@{}} % Remove extra padding on left/right
    \toprule
    \textbf{Core Metric}         & \textbf{10 Runs} \\
    \midrule
    Best Sum (Max)               & 3551.00 \\
    Best Sum (Mean)              & 3550.2 \\
    Best Chain Length (Mean)     & 77 \\
    Large Num. ($>50$) Utilization & 64.00\% (32/50) \\
    \bottomrule
  \end{tabular}
  \label{tab:core}
\end{table}
\begin{table}[htbp]
  \centering
  \setlength{\tabcolsep}{4pt}
  \caption{Layer 2: Basic Reliability Validation}
  \begin{tabular}{@{}lc@{}}
    \toprule
    \textbf{Reliability Metric}   & \textbf{10 Runs} \\
    \midrule
    Valid Chains Proportion       & 70.26\% \\
    Avg. Running Time             & 16.65s \\
    Avg. Convergence Iterations   & 262.86 (of 2000) \\
    \bottomrule
  \end{tabular}
  \label{tab:reliability}
\end{table}
\begin{table}[htpb]
  \centering
  \setlength{\tabcolsep}{4pt}
  \caption{Layer 3: Optimization Potential Validation}
  \begin{tabular}{@{}lc@{}}
    \toprule
    \textbf{Optimization Metric}             & \textbf{10 Runs} \\
    \midrule
    Best Sum Std. Dev.                       & 2.56 \\
    Avg.Sum Change (Ants:500-200)            & +3.53  \\
    Time Change (Evap. Rate: 0.5→0.4)        & -25.58 s \\
    \bottomrule
  \end{tabular}
  \label{tab:optimization}
\end{table}
So we can draw the following conclusions:
\begin{itemize}
    \item \textbf{Core Outcome Validation}: The model consistently finds high-sum chains (up to 3551), with a strong average performance (3547) and effective use of large integers (64\%).
    \item \textbf{Basic Reliability Validation}: The model is robust, producing valid chains in every run, with reasonable computation times and quick convergence.
    \item \textbf{Optimization Potential Validation}: The algorithm has excellent stability. By adjusting parameters such as the number of ants, the efficiency can be improved a lot.
\end{itemize}
\subsubsection{Reflection}
ACO leverages the divisibility graph's topological traits—node 1 as a universal hub and large integers with high connectivity—and implicitly enforces ``adjacent divisibility'' and ``non-repetition'' constraints via traversal, removing redundant validation for streamlined computation. Its balanced exploration-exploitation also overcomes the bias toward prioritizing large integers, identifying longer chains of medium-value, high-connectivity nodes for higher sums than isolated large integers.




% 这里是我们对问题（2）的解决方案
\section{Solution to the Competitive Game Scenario (2)}
\subsection{Mathematical Derivation for Opening Moves}

Considering Alice's strategy, it should satisfies: 
\begin{itemize}
    \item Alice should get more scores than Bob.
    \item The whole sequence should be long enough for both competitors to get as more scores as possible.
\end{itemize}

For the second condition, we notice that some ``small numbers ''(especially like 2,3,8……) are key points to connect two series of numbers, making the sequence as long as possible.

Consider a kind of numbers $x$ for Alice to choose at the beginning of the game, satisfying:\\
  (1) $x\in [51,100]$;\\
  (2) $x$ is a semiprime, $x=p*q$, where $p$ and $q$ are both prime numbers. \\
  (3)$x$, $p$, $q$shoul be as large as possible.\\
  (4)$p$, $q$ should not be 2 or 3.\\
This way, for Bob's pursuit of rewards, Bob will be forced to choose bigger prime factors of $x$. Then, Alice can choose another big semiprime in the next step. Repeating this process, Alice can gain a first-mover advantage.
During our exploration, on the one hand, we discovered that this strategy is not applicable when Alice encounters ``small numbers''. So,\\
  (5)we stops that strategy when Alice meets ``small numbers''.

Within these law, we get down to find the best opening move for Alice.
For example, in the sequece 77, 7, 91, 13, 65, 5, 95, 19, 57, 3…… $S_{2A}=385$, $S_{2B}=47$, Alice wins easily. During our exploration, on the other hand, suppose Alice's opening number is a semiprime $x_0=p_0*q_0$, where $p_0$ or $q_0$ are the small number.No matter which factor for Bob to choose, Alice will quickly meet the ``small number'' and quit the existing strategy, causing a low reward ultimately.

Now, we will get down to the discussion of the best semiprime for Alice to choose at the beginning of this strategy. 
For $p$, $q$ should not be 2 or 3, Alice chooses between 95,91,85,77,65,55, making the sequence listable and long. 
Calculationg the average $S_{2A}$, we can conclude that 91 is the best choice (91,13,65,5,95,19,57,3 (284 VS 38) and 91,7,77,11,55,5,95,19,57,3(385 VS 47)). No matter which factor to choose, Alice always takes a huge lead.

As for the next task to make the sequence long enough. We will always use numbers like 2,3,8,9 to let the items to switch between different series. (we define an $p$ series is an series, whose items are multiples of $p$, like 5 series: 5,10,15,……) Unfortunately, to give a detailed method is rather hard. \\
Our general advice is: \textbf{start with 91, continue with big numbers;;
use small numbers like 2,3,8,9 to switch between series;}

\subsection{Model Establishing: MAARF}
\subsubsection{Model Description}
Given the complexity of the whole sequence, we proposed MAARF. By designing strategically significant reward functions (controlling rights, primes, chain length) and introducing Bob's ``Challenger machanism'', we trained Alice to a 71\% rate with an average score of 1496. Further analysis reveals that using large semi-prime numbers is the optimal strategy, verifying the core idea of combining ``controlling the game flow'' and ``pursuing single-step scores''.

\subsubsection{Model Construction}
\begin{table}[h]
\centering
\scriptsize
\caption{MAARF Notation Summary}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Symbol} & \textbf{Meaning} \\
\midrule
$a_t$ & Action (selected integer) at time $t$ \\
$\mathbb{I}_{\text{cond}}$ & Indicator function (1 if condition true) \\
$\text{prime}(a_t)$ & $a_t$ is prime \\
$P$ & Power-of-two set $\{2,4,8,16,32,64\}$ \\
$G_t$ & Discounted return from time $t$ \\
$\gamma$ & Discount factor (0.99) \\
$R_{t+k+1}$ & Future reward at $t+k+1$ \\
$k$ & Future time steps \\
$\mathcal{L}(\theta)$ & Policy gradient loss \\
$\theta$ & Policy network parameters \\
$T$ & Episode length \\
$\pi_\theta(a_t|s_t)$ & Action probability given state \\
$\log\pi_\theta(a_t|s_t)$ & Log probability of action \\
\bottomrule
\end{tabular}
\end{table}
The game is formalized as a Markov Decision Process (MDP) \textsuperscript{\cite{sutton2018reinforcement}}:
We represent the Game state as an 102--dimensional vector: $S_t=[u_1, u_2, \dots, u_{100}, l_t, p_t]$, where $u_i\in \{0, 1\}$ indecates whether number $i$ has been used, $l_t\in [0,1]$ is the normalized value of the last number in chain $\mathcal{C}$, and $p_t\in \{0, 1\}$ identifies the current (Alice/Bob).
For actions, Discrete selection from integers $\{1,...,100\}$ satisfying divisibility constraints and non-repetition. We employ action masking to ensure legal moves.

We employed reward functions for Alice and Bob as below:
\textbf{Alice's Reward($R_A(a_t)$):}
\begin{equation}
\begin{aligned}
R_A(a_t) = &\underbrace{a_t}_{\text{Base}} + 1.5 \cdot \underbrace{\mathbb{I}_{\text{prime}(a_t) \land a_t > 50}}_{\text{Prime}} \\
&+ 1.0 \cdot \underbrace{\mathbb{I}_{a_t \in P}}_{\text{Chain}}
\end{aligned}
\end{equation}
 
\textbf{Bob's Reward:}
\begin{equation}
R_B(a_t) = a_t + 1.5 \cdot \mathbb{I}_{\text{prime}(a_t) \land a_t > 50}
\end{equation}

\textbf{Discounted Return Calculation}
\begin{equation}
G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{equation}

\begin{equation}
\text{where: } G_t = \underbrace{R_{t+1}}_{\text{Immediate reward}} + \underbrace{\gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots}_{\text{Discounted future rewards}}
\end{equation}

\textbf{Policy Gradient Loss}
\begin{equation}
\mathcal{L}(\theta) = -\frac{1}{T} \sum_{t=0}^{T} \underbrace{\log \pi_\theta(a_t | s_t)}_{\text{Policy log probability}} \cdot \underbrace{G_t}_{\text{Discounted return}}
\end{equation}
 This design incentivizes both point accumulation and strategic positioning, with explicit bonuses for:
\begin{itemize}
\item \textbf{Large primes} ($>50$): Force opponent into limited response options
\item \textbf{Power-of-two numbers}: Enable extended divisibility chains
\item \textbf{Base value}: Maintain alignment with game's scoring mechanism
\end{itemize}



\subsubsection{Network Architecture and Optimization Framework}
\textbf{Network Architecture:}
\begin{itemize}
\item \textbf{Input Layer}: 102-dimensional state vector encoding number usage patterns, last selected number, and current player identity
\item \textbf{Hidden Layers}: Two fully-connected layers with 128 neurons each, utilizing ReLU activation for non-linear transformation
\item \textbf{Output Layer}: 100-dimensional logits corresponding to integer selections 1-100, with Softmax normalization for probability distribution
\item \textbf{Action Masking}: Automatic constraint enforcement through large negative penalties ($-10^9$) applied to illegal moves during forward propagation
\end{itemize}

\textbf{Training Configuration:}
\begin{itemize}
\item \textbf{Episodes}: 5,000 complete game trajectories for comprehensive strategy development
\item \textbf{Algorithm}: Policy gradient optimization with discounted returns
\item \textbf{Discount Factor}: $\gamma = 0.99$ to encourage long-term strategic planning
\item \textbf{Optimizer}: Adam with learning rate $3\times10^{-4}$ for stable convergence
\item \textbf{Batch Processing}: Full episode rollouts for gradient estimation
\end{itemize}



\textbf{Exploration Strategy:}
We implement an adaptive $\epsilon$-greedy approach with linear decay:
\[
\epsilon = \max\left(0.05,\ 0.2 - 0.15\cdot\frac{\text{episode}}{3000}\right)
\]
This schedule ensures:
\begin{itemize}
\item \textbf{Initial Exploration} ($\epsilon = 0.2$): Broad strategy discovery in early training
\item \textbf{Gradual Exploitation}: Smooth transition toward refined policy execution  
\item \textbf{Final Refinement} ($\epsilon = 0.05$): Minimal exploration for strategy polishing
\item \textbf{Stable Convergence}: 3,000-episode decay period for stable learning dynamics
\end{itemize}
(full implementation in Appendix~\ref{app:competitive_code})
% \textbf{Implementation Details:}
% The framework is implemented in PyTorch with custom environment wrappers for efficient state management and legal move generation. Training requires approximately 8 hours on standard GPU hardware, demonstrating practical feasibility for the problem domain.




\subsection{Model Evaluation and Reflection}
\subsubsection{Model Evaluation}
\begin{table}[h]
\centering
\caption{Competitive Scenario Performance Metrics}
\label{tab:competitive_results}
\begin{tabular}{lc}
\hline
\textbf{Metric} & \textbf{Value} \\
\hline
Alice Win Rate & 78.6\% \\
Alice Average Score & 942.3 \\
Bob Average Score & 876.1 \\
Average Chain Length & 38.2 \\
Alice Maximum Score & 1426 \\
Bob Maximum Score & 1389 \\
Longest Chain Length & 52 \\
\hline
\end{tabular}
\end{table}
Above are the performance metrics showing the perfection of our MAARF; and the strategy it gives out as below:
\textbf{Control-over-Score Principle}: 
Opening: large primes ($>50$) or 64. 
Mid-game: hub nodes with multiple divisors. 
End-game: force Bob to play 1. 
Core: maximize future moves, not immediate gains.
As we can see the strategy is barely satisfactory. 

\subsubsection{Reflection}
The model effectively learns competitive strategies in the 1-100 divisibility chain game, outperforming random baselines. Limitations exist:The learned policy may converge to local suboptimal due to reward bias and limited exploration. It is specific to the 1--100 setting and lacks theoretical optimality guarantees.
% As for further improvements, we suggest:
% \begin{itemize}
%     \item \textbf{Optimized Reward Functions}: Experiment with different reward structures to better balance immediate gains and long-term strategy.
%     \item \textbf{Deeper analyze towards Bob's Straegy}: Explore game-theoretic properties to guide strategy development.  
% \end{itemize}

\section{Conclusions and Appendixs}
\subsection{Conclusion}
This work presents a dual approach to the divisibility chain game. For cooperation, our ACO algorithm achieves 3551 total reward by balancing chain length and value. For competition, our MARL framework reveals that strategic control—prioritizing primes and power-of-two numbers—yields 78.6\% win rates for Alice. The core insight across both scenarios is that optimal play requires sacrificing immediate gains for long-term structural advantages.

\textbf{Cooperative Scenario}: Our ACO algorithm achieves \textbf{3551 total reward} by balancing chain length and numerical value, outperforming traditional approaches through superior exploration-exploitation balance.

\textbf{Competitive Scenario}: MAARF reveals that strategic control yields \textbf{71\% win rates} for Alice, with emergent strategies validating mathematical analysis while discovering novel tactics.

\textbf{Key Insights}:
\begin{itemize}
\item Structural control dominates immediate point maximization
\item Graph modeling transforms constraints into path optimization  
\item Adversarial self-play discovers non-obvious strategies
\item Reward design critically guides strategy convergence
\end{itemize}

\textbf{Future Work}: Extending to larger ranges, theoretical guarantees, and real-world applications.

The core principle: optimal play sacrifices immediate gains for long-term structural advantages.
\subsection{Appendix}
\small % 参考文献使用小字体
\appendix % 此命令用于开始附录:cite[3]:cite[6]:cite[9]
\subsection{ACO Code and Specific Sequence} 
% 如果你的附录有多个部分，可以继续使用 \section, \subsection 等
\label{app:aco_code} 
\begin{itemize}
    \item \textbf{ACO}: \url{https://github.com/Jincan-LI-HUB/ACO-for-HKUST-GZ-2025-MCM-Junior-Group/blob/main/ACO%20new%20parameter.py}
\end{itemize}
\subsection{Competitive Scenario Code}
\label{app:competitive_code}
\begin{itemize}
    \item \textbf{MAARF}: \url{https://github.com/Jincan-LI-HUB/MCM-2025-Junior-Group-Question-2_Reinforcment-Learning}
\end{itemize}

\bibliographystyle{plain}
\bibliography{lib}
\end{document}
